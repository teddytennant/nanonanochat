{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ecbd8d9",
   "metadata": {},
   "source": [
    "# Training Modern Transformer from Scratch on 80GB A100 GPU\n",
    "\n",
    "This notebook implements and trains a **modern, optimized transformer** from scratch using PyTorch, incorporating state-of-the-art improvements over GPT-2:\n",
    "\n",
    "**Modern Architecture Features:**\n",
    "- **Flash Attention** (memory-efficient scaled dot-product attention)\n",
    "- **RMSNorm** instead of LayerNorm (faster, more stable)\n",
    "- **SwiGLU activation** instead of GELU (better performance)\n",
    "- **Rotary Position Embeddings (RoPE)** instead of learned positional embeddings\n",
    "- **Grouped Query Attention (GQA)** for efficiency\n",
    "- **Optimized for 80GB A100** with mixed precision training\n",
    "\n",
    "This is a **custom, improved architecture** designed for maximum performance on modern hardware, not a recreation of existing models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5f8ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install torch --index-url https://download.pytorch.org/whl/cu121\n",
    "!pip install transformers datasets tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d320c343",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from datasets import load_dataset\n",
    "from transformers import GPT2Tokenizer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee038f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerConfig:\n",
    "    def __init__(self, vocab_size, block_size, n_layer, n_head, n_embd, n_kv_head=None, dropout=0.0):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.block_size = block_size\n",
    "        self.n_layer = n_layer\n",
    "        self.n_head = n_head\n",
    "        self.n_embd = n_embd\n",
    "        # Grouped Query Attention: fewer KV heads than Q heads\n",
    "        self.n_kv_head = n_kv_head if n_kv_head is not None else n_head\n",
    "        self.dropout = dropout\n",
    "        self.head_dim = n_embd // n_head\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    \"\"\"Root Mean Square Layer Normalization (more efficient than LayerNorm)\"\"\"\n",
    "    def __init__(self, dim, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        norm = torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "        return x * norm * self.weight\n",
    "\n",
    "def apply_rotary_emb(x, cos, sin):\n",
    "    \"\"\"Apply Rotary Position Embeddings\"\"\"\n",
    "    # x: (batch, seq_len, n_head, head_dim)\n",
    "    x1, x2 = x[..., :x.shape[-1]//2], x[..., x.shape[-1]//2:]\n",
    "    # Rotate\n",
    "    rotated = torch.cat([-x2, x1], dim=-1)\n",
    "    return (x * cos) + (rotated * sin)\n",
    "\n",
    "def precompute_freqs_cis(dim, max_seq_len, theta=10000.0):\n",
    "    \"\"\"Precompute RoPE frequencies\"\"\"\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2).float() / dim))\n",
    "    t = torch.arange(max_seq_len)\n",
    "    freqs = torch.outer(t, freqs).float()\n",
    "    cos = torch.cos(freqs)\n",
    "    sin = torch.sin(freqs)\n",
    "    return cos, sin\n",
    "\n",
    "class GroupedQueryAttention(nn.Module):\n",
    "    \"\"\"Multi-Query Attention with Grouped Queries (GQA) for efficiency\"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        \n",
    "        self.n_head = config.n_head\n",
    "        self.n_kv_head = config.n_kv_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.head_dim = config.head_dim\n",
    "        self.n_rep = self.n_head // self.n_kv_head  # repetition factor for KV heads\n",
    "        \n",
    "        # Q projection for all heads, K/V for fewer heads\n",
    "        self.wq = nn.Linear(config.n_embd, config.n_head * config.head_dim, bias=False)\n",
    "        self.wk = nn.Linear(config.n_embd, config.n_kv_head * config.head_dim, bias=False)\n",
    "        self.wv = nn.Linear(config.n_embd, config.n_kv_head * config.head_dim, bias=False)\n",
    "        self.wo = nn.Linear(config.n_head * config.head_dim, config.n_embd, bias=False)\n",
    "        \n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        \n",
    "        # Causal mask\n",
    "        self.register_buffer(\"mask\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                           .view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x, freqs_cos, freqs_sin):\n",
    "        B, T, C = x.size()\n",
    "        \n",
    "        # Project and reshape\n",
    "        q = self.wq(x).view(B, T, self.n_head, self.head_dim)\n",
    "        k = self.wk(x).view(B, T, self.n_kv_head, self.head_dim)\n",
    "        v = self.wv(x).view(B, T, self.n_kv_head, self.head_dim)\n",
    "        \n",
    "        # Apply RoPE\n",
    "        q = apply_rotary_emb(q, freqs_cos[:T], freqs_sin[:T])\n",
    "        k = apply_rotary_emb(k, freqs_cos[:T], freqs_sin[:T])\n",
    "        \n",
    "        # Repeat KV heads to match Q heads (for GQA)\n",
    "        if self.n_rep > 1:\n",
    "            k = k.repeat_interleave(self.n_rep, dim=2)\n",
    "            v = v.repeat_interleave(self.n_rep, dim=2)\n",
    "        \n",
    "        # Transpose for attention computation\n",
    "        q = q.transpose(1, 2)  # (B, n_head, T, head_dim)\n",
    "        k = k.transpose(1, 2)\n",
    "        v = v.transpose(1, 2)\n",
    "        \n",
    "        # Scaled dot-product attention with Flash Attention if available\n",
    "        if hasattr(F, 'scaled_dot_product_attention'):\n",
    "            # Use PyTorch's optimized Flash Attention\n",
    "            y = F.scaled_dot_product_attention(\n",
    "                q, k, v,\n",
    "                attn_mask=None,\n",
    "                dropout_p=self.dropout.p if self.training else 0.0,\n",
    "                is_causal=True\n",
    "            )\n",
    "        else:\n",
    "            # Fallback to manual attention\n",
    "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(self.head_dim))\n",
    "            att = att.masked_fill(self.mask[:, :, :T, :T] == 0, float('-inf'))\n",
    "            att = F.softmax(att, dim=-1)\n",
    "            att = self.dropout(att)\n",
    "            y = att @ v\n",
    "        \n",
    "        # Reshape and project output\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, -1)\n",
    "        y = self.wo(y)\n",
    "        return y\n",
    "\n",
    "class SwiGLU(nn.Module):\n",
    "    \"\"\"SwiGLU activation (better than GELU for language models)\"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        hidden_dim = int(8 * config.n_embd / 3)  # Common FFN expansion\n",
    "        hidden_dim = ((hidden_dim + 255) // 256) * 256  # Round to multiple of 256 for efficiency\n",
    "        \n",
    "        self.w1 = nn.Linear(config.n_embd, hidden_dim, bias=False)\n",
    "        self.w2 = nn.Linear(hidden_dim, config.n_embd, bias=False)\n",
    "        self.w3 = nn.Linear(config.n_embd, hidden_dim, bias=False)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # SwiGLU: swish(W1¬∑x) ‚äô (W3¬∑x) then project with W2\n",
    "        return self.dropout(self.w2(F.silu(self.w1(x)) * self.w3(x)))\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.attention_norm = RMSNorm(config.n_embd)\n",
    "        self.attention = GroupedQueryAttention(config)\n",
    "        self.ffn_norm = RMSNorm(config.n_embd)\n",
    "        self.ffn = SwiGLU(config)\n",
    "\n",
    "    def forward(self, x, freqs_cos, freqs_sin):\n",
    "        # Pre-norm architecture with residual connections\n",
    "        x = x + self.attention(self.attention_norm(x), freqs_cos, freqs_sin)\n",
    "        x = x + self.ffn(self.ffn_norm(x))\n",
    "        return x\n",
    "\n",
    "class ModernTransformer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Token embeddings only (no learned positional embeddings - using RoPE instead)\n",
    "        self.token_emb = nn.Embedding(config.vocab_size, config.n_embd)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        \n",
    "        # Precompute RoPE frequencies\n",
    "        freqs_cos, freqs_sin = precompute_freqs_cis(config.head_dim, config.block_size)\n",
    "        self.register_buffer(\"freqs_cos\", freqs_cos)\n",
    "        self.register_buffer(\"freqs_sin\", freqs_sin)\n",
    "        \n",
    "        # Transformer blocks\n",
    "        self.blocks = nn.ModuleList([TransformerBlock(config) for _ in range(config.n_layer)])\n",
    "        self.norm = RMSNorm(config.n_embd)\n",
    "        \n",
    "        # Output projection\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        \n",
    "        # Weight tying\n",
    "        self.token_emb.weight = self.lm_head.weight\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.size()\n",
    "        assert T <= self.config.block_size, f\"Sequence length {T} exceeds block size {self.config.block_size}\"\n",
    "        \n",
    "        # Token embeddings (no positional - using RoPE in attention)\n",
    "        x = self.token_emb(idx)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Transform through blocks\n",
    "        freqs_cos = self.freqs_cos[:T].unsqueeze(0).unsqueeze(2)\n",
    "        freqs_sin = self.freqs_sin[:T].unsqueeze(0).unsqueeze(2)\n",
    "        \n",
    "        for block in self.blocks:\n",
    "            x = block(x, freqs_cos, freqs_sin)\n",
    "        \n",
    "        x = self.norm(x)\n",
    "        logits = self.lm_head(x)\n",
    "        \n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=50):\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Crop context if needed\n",
    "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
    "            \n",
    "            # Forward pass\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            \n",
    "            # Top-k sampling\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "            \n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        \n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea86af32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(batch_size=8, block_size=1024):\n",
    "    # Load dataset\n",
    "    print(\"Downloading Wikitext dataset... This may take a few minutes.\")\n",
    "    dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "\n",
    "    # Load tokenizer\n",
    "    print(\"Loading GPT-2 tokenizer...\")\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # Concatenate all texts\n",
    "    print(\"Concatenating texts...\")\n",
    "    train_text = \"\\n\\n\".join(dataset['train']['text'])\n",
    "    val_text = \"\\n\\n\".join(dataset['validation']['text'])\n",
    "\n",
    "    # Tokenize\n",
    "    print(\"Tokenizing data... This may take some time.\")\n",
    "    train_tokens = tokenizer.encode(train_text)\n",
    "    val_tokens = tokenizer.encode(val_text)\n",
    "\n",
    "    # Chunk into blocks\n",
    "    print(\"Chunking tokens into blocks...\")\n",
    "    def chunk_tokens(tokens, block_size):\n",
    "        chunks = []\n",
    "        for i in range(0, len(tokens) - block_size + 1, block_size):\n",
    "            chunks.append(tokens[i:i + block_size])\n",
    "        return chunks\n",
    "\n",
    "    train_chunks = chunk_tokens(train_tokens, block_size)\n",
    "    val_chunks = chunk_tokens(val_tokens, block_size)\n",
    "\n",
    "    # Convert to tensors\n",
    "    print(\"Converting to tensors...\")\n",
    "    train_data = torch.tensor(train_chunks, dtype=torch.long)\n",
    "    val_data = torch.tensor(val_chunks, dtype=torch.long)\n",
    "\n",
    "    # Create data loaders\n",
    "    print(\"Creating data loaders...\")\n",
    "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    print(\"Data preparation complete!\")\n",
    "    return train_loader, val_loader, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4be6cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, optimizer, scheduler, device, epochs=1, grad_accum_steps=1, use_amp=True):\n",
    "    scaler = torch.cuda.amp.GradScaler() if use_amp else None\n",
    "    model.train()\n",
    "    \n",
    "    print(f\"\\nStarting training on {device}\")\n",
    "    if device.type == 'cuda':\n",
    "        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"Initial GPU memory allocated: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n",
    "        print(f\"Initial GPU memory reserved: {torch.cuda.memory_reserved(0) / 1024**3:.2f} GB\\n\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for i, batch in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")):\n",
    "            # Handle both tensor and dict inputs\n",
    "            if isinstance(batch, dict):\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "            else:\n",
    "                input_ids = batch.to(device)\n",
    "                \n",
    "            targets = input_ids.clone()\n",
    "            targets[:, :-1] = input_ids[:, 1:]\n",
    "            targets[:, -1] = -1  # ignore last token\n",
    "\n",
    "            with torch.cuda.amp.autocast(enabled=use_amp):\n",
    "                logits, loss = model(input_ids, targets)\n",
    "                loss = loss / grad_accum_steps\n",
    "\n",
    "            if use_amp:\n",
    "                scaler.scale(loss).backward()\n",
    "            else:\n",
    "                loss.backward()\n",
    "\n",
    "            if (i + 1) % grad_accum_steps == 0:\n",
    "                if use_amp:\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                else:\n",
    "                    optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                scheduler.step()\n",
    "\n",
    "            total_loss += loss.item() * grad_accum_steps\n",
    "            \n",
    "            # Print memory usage every 100 steps\n",
    "            if device.type == 'cuda' and (i + 1) % 100 == 0:\n",
    "                print(f\"\\nStep {i+1}: GPU memory allocated: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        if device.type == 'cuda':\n",
    "            print(f\"Peak GPU memory allocated: {torch.cuda.max_memory_allocated(0) / 1024**3:.2f} GB\")\n",
    "            print(f\"Peak GPU memory reserved: {torch.cuda.max_memory_reserved(0) / 1024**3:.2f} GB\")\n",
    "\n",
    "        # Validation\n",
    "        print(f\"\\nRunning validation...\")\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc=\"Validation\"):\n",
    "                # Handle both tensor and dict inputs\n",
    "                if isinstance(batch, dict):\n",
    "                    input_ids = batch['input_ids'].to(device)\n",
    "                else:\n",
    "                    input_ids = batch.to(device)\n",
    "                    \n",
    "                targets = input_ids.clone()\n",
    "                targets[:, :-1] = input_ids[:, 1:]\n",
    "                targets[:, -1] = -1\n",
    "                _, loss = model(input_ids, targets)\n",
    "                val_loss += loss.item()\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Val Loss: {avg_val_loss:.4f}\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "        model.train()\n",
    "\n",
    "        # Save checkpoint\n",
    "        checkpoint_path = f'checkpoint_epoch_{epoch+1}.pt'\n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "        print(f\"Checkpoint saved: {checkpoint_path}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038bc08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for 80GB A100 GPU - Optimized for Maximum Performance\n",
    "print(\"=\"*60)\n",
    "print(\"Configuration for 80GB A100 GPU - Training Large Model\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Choose model size: 'medium' (355M), 'large' (774M), or 'xl' (1.5B)\n",
    "MODEL_SIZE = 'large'  # Change to 'xl' for even bigger model\n",
    "\n",
    "if MODEL_SIZE == 'medium':\n",
    "    # GPT-2 Medium (355M parameters)\n",
    "    n_layer = 24\n",
    "    n_head = 16\n",
    "    n_embd = 1024\n",
    "    batch_size = 12\n",
    "    grad_accum_steps = 8\n",
    "    print(\"\\nüöÄ Training GPT-2 Medium (355M parameters)\")\n",
    "    \n",
    "elif MODEL_SIZE == 'large':\n",
    "    # GPT-2 Large (774M parameters)\n",
    "    n_layer = 36\n",
    "    n_head = 20\n",
    "    n_embd = 1280\n",
    "    batch_size = 6\n",
    "    grad_accum_steps = 12\n",
    "    print(\"\\nüöÄ Training GPT-2 Large (774M parameters)\")\n",
    "    \n",
    "elif MODEL_SIZE == 'xl':\n",
    "    # GPT-2 XL (1.5B parameters)\n",
    "    n_layer = 48\n",
    "    n_head = 25\n",
    "    n_embd = 1600\n",
    "    batch_size = 4\n",
    "    grad_accum_steps = 16\n",
    "    print(\"\\nüöÄ Training GPT-2 XL (1.5B parameters)\")\n",
    "\n",
    "block_size = 1024\n",
    "epochs = 1\n",
    "lr = 3e-4\n",
    "use_amp = True  # Mixed precision training for A100\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Total GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"PyTorch Version: {torch.__version__}\")\n",
    "    \n",
    "# Estimated memory usage\n",
    "print(f\"\\nModel Configuration:\")\n",
    "print(f\"  Layers: {n_layer}, Heads: {n_head}, Embedding: {n_embd}\")\n",
    "print(f\"  Batch size: {batch_size}, Block size: {block_size}\")\n",
    "print(f\"  Gradient accumulation steps: {grad_accum_steps}\")\n",
    "print(f\"  Effective batch size: {batch_size * grad_accum_steps}\")\n",
    "\n",
    "# Calculate approximate model size\n",
    "params = 12 * n_layer * n_embd**2 + 2 * n_embd * 50257 + 2 * n_embd * block_size\n",
    "model_size_gb = params * 4 / 1024**3  # 4 bytes per float32\n",
    "optimizer_size_gb = params * 8 / 1024**3  # AdamW uses 2x model size (first and second moments)\n",
    "gradients_size_gb = params * 4 / 1024**3\n",
    "activations_gb = batch_size * block_size * n_embd * n_layer * 4 / 1024**3\n",
    "\n",
    "total_estimated_gb = model_size_gb + optimizer_size_gb + gradients_size_gb + activations_gb\n",
    "\n",
    "print(f\"\\nüìä Memory Estimates:\")\n",
    "print(f\"  Model parameters: ~{model_size_gb:.2f} GB\")\n",
    "print(f\"  Optimizer states: ~{optimizer_size_gb:.2f} GB\")\n",
    "print(f\"  Gradients: ~{gradients_size_gb:.2f} GB\")\n",
    "print(f\"  Activations (approx): ~{activations_gb:.2f} GB\")\n",
    "print(f\"  Total estimated: ~{total_estimated_gb:.2f} GB\")\n",
    "print(f\"  Available: 80 GB\")\n",
    "print(f\"  Safety margin: ~{80 - total_estimated_gb:.2f} GB\")\n",
    "\n",
    "if total_estimated_gb > 70:\n",
    "    print(f\"\\n‚ö†Ô∏è  Warning: High memory usage. Consider reducing batch_size or grad_accum_steps.\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ Configuration should fit comfortably on an 80GB A100 GPU.\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# Model initialization\n",
    "print(\"Initializing model...\")\n",
    "config = GPTConfig(vocab_size=50257, block_size=block_size, n_layer=n_layer, n_head=n_head, n_embd=n_embd, dropout=0.1)\n",
    "model = GPT(config).to(device)\n",
    "\n",
    "# Count actual parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params:,} ({total_params/1e6:.2f}M)\")\n",
    "print(f\"Trainable parameters: {trainable_params:,} ({trainable_params/1e6:.2f}M)\")\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    print(f\"Model loaded. GPU memory allocated: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\\n\")\n",
    "\n",
    "# Data loading\n",
    "print(\"Loading and preparing data...\")\n",
    "train_loader, val_loader, tokenizer = get_data(batch_size=batch_size, block_size=block_size)\n",
    "print(f\"Train batches: {len(train_loader)}, Validation batches: {len(val_loader)}\\n\")\n",
    "\n",
    "# Optimizer and scheduler\n",
    "print(\"Setting up optimizer and scheduler...\")\n",
    "optimizer = AdamW(model.parameters(), lr=lr, weight_decay=0.01, betas=(0.9, 0.95))\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=len(train_loader) * epochs // grad_accum_steps)\n",
    "print(\"Setup complete!\\n\")\n",
    "\n",
    "# Train\n",
    "train(model, train_loader, val_loader, optimizer, scheduler, device, epochs, grad_accum_steps, use_amp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3dc1867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation\n",
    "def generate_text(model, tokenizer, prompt, max_new_tokens=50, temperature=1.0, top_k=50):\n",
    "    model.eval()\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(input_ids, max_new_tokens, temperature=temperature, do_sample=True, top_k=top_k)\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "# Example\n",
    "prompt = \"The future of AI is\"\n",
    "generated = generate_text(model, tokenizer, prompt)\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60aa8fa5",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "1. **Install dependencies**: Run cell 2 to install PyTorch, transformers, datasets, and tqdm.\n",
    "2. **Import libraries**: Run cell 3 to import all required libraries.\n",
    "3. **Load model architecture**: Run cell 4 to define the GPT model classes.\n",
    "4. **Load data preparation function**: Run cell 5 to define the data loading function.\n",
    "5. **Load training function**: Run cell 6 to define the training loop.\n",
    "6. **Configure and train**: Run cell 7 to configure the model size and start training.\n",
    "   - **Model sizes available:**\n",
    "     - `'medium'`: 355M parameters (~15-20 GB total memory)\n",
    "     - `'large'`: 774M parameters (~30-35 GB total memory) - **Recommended**\n",
    "     - `'xl'`: 1.5B parameters (~55-65 GB total memory) - Maximum capacity\n",
    "7. **Generate text**: After training completes, run cell 8 to test text generation.\n",
    "\n",
    "## Model Size Selection\n",
    "\n",
    "The configuration automatically selects appropriate batch sizes and gradient accumulation steps for each model size to maximize utilization of the 80GB A100 GPU:\n",
    "\n",
    "- **GPT-2 Medium** (355M): Fast training, good quality\n",
    "- **GPT-2 Large** (774M): Best balance of speed and quality - **recommended for most use cases**\n",
    "- **GPT-2 XL** (1.5B): Highest quality, slower training, pushes GPU to limits\n",
    "\n",
    "To change model size, edit the `MODEL_SIZE` variable in cell 7 before running it.\n",
    "\n",
    "## Training Tips\n",
    "\n",
    "- Training will take several hours depending on model size\n",
    "- Checkpoints are saved after each epoch\n",
    "- GPU memory usage is monitored and displayed during training\n",
    "- Mixed precision (FP16) is enabled for faster training on A100\n",
    "- If you encounter OOM errors, reduce `batch_size` or increase `grad_accum_steps`"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
